# Base Models

Within the domain of LLMs, the term "base model" denotes the initial iteration, meticulously honed through extensive training on extensive datasets utilizing deep learning methodologies. Following this foundational training, it can be tailored to execute specific tasks, such as text classification, question-answering, summarization, and text generation. Base model constitutes the fundamental framework underpinning practical AI applications.

<br />

import Image from "next/image";

<div style={{ position: "relative" }}>
  <Image
    src="/static/img/osLLm.png"
    alt="Open-source LLM img"
    height="300"
    width="1000"
    priority
  />
</div>

### Models

| Models                                                      | Developed by                                                                 | Parameter                                                                                                                                                                                                                                                                                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ----------------------------------------------------------- | ---------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Mistral](https://mistral.ai/product/)                      | [Mistral AI](https://mistral.ai/)                                            | [Mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Mistral-7B-Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)                                                                                                                                                                                       | Mistral-7B-v0.1 is a small, yet powerful model adaptable to many use-cases. It outperforms Llama 2 13B on all benchmark assessments, possesses inherent natural coding capabilities, and 8k sequence length. Released under the Apache 2.0 license, it has been designed for effortless deployment on a wide range of cloud platforms, ensuring accessibility and convenience for users.                                                                                                                                                                                                                                            |
| [Falcon](https://falconllm.tii.ae/)                         | [Technology Innovation Institute (TII)](https://falconllm.tii.ae/index.html) | [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b), [Falcon-180B](https://huggingface.co/tiiuae/falcon-180B)                                                                                                                                                                 | The Falcon LLM has garnered acclaim as an advanced technology capable of both comprehending and generating human language, boasting versatile applications across diverse industries. It has ascended to a position of prominence, surpassing its predecessor, LLaMA, another substantial language model, and is now widely regarded as the preeminent leader within the realm of open-source LLMs.                                                                                                                                                                                                                                 |
| [LLaMA 2](https://ai.meta.com/llama/)                       | [Meta AI](https://ai.meta.com/)                                              | [LLaMa2-7B](https://huggingface.co/meta-llama/Llama-2-7b), [LLaMa2-13B](https://huggingface.co/meta-llama/Llama-2-13b-hf), [LLaMa2-70B](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)                                                                                                                                         | Designed to help researchers & developers in the field of AI by providing a versatile and efficient LLM, trained on a wide range of text sources, including Common Crawl, GitHub, Wikipedia, Project Gutenberg, ArXiv, and Stack Exchange.                                                                                                                                                                                                                                                                                                                                                                                          |
| [MPT](https://www.mosaicml.com/mpt)                         | [Mosaic ML](https://www.mosaicml.com/)                                       | [MPT-7B](https://huggingface.co/mosaicml/mpt-7b)                                                                                                                                                                                                                                                                                       | It is a decoder-style transformer that has been pretrained from scratch on 1T tokens of English text and code, available for commercial use, Fast training and inference, Handling long inputs.                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)         | [LMSYS ORG](https://lmsys.org/)                                              | [Vicuna-7B](https://huggingface.co/lmsys/vicuna-7b-v1.5), [Vicuna-13B](https://huggingface.co/lmsys/vicuna-13b-v1.5-16k), [Vicuna-33B](https://huggingface.co/lmsys/vicuna-33b-v1.3)                                                                                                                                                   | Vicuna is an open-source chatbot that has undergone fine-tuning using user-shared conversations gathered from ShareGPT. In initial assessments, where GPT-4 served as the evaluator, Vicuna-13B demonstrated a quality level of over 90%, surpassing both OpenAI ChatGPT and Google Bard.                                                                                                                                                                                                                                                                                                                                           |
| [Bloom](https://huggingface.co/bigscience/bloom)            | [BigScience](https://bigscience.huggingface.co/)                             | [Bloom-176B](https://huggingface.co/bigscience/bloom)                                                                                                                                                                                                                                                                                  | An open-source multilingual LLM that can generate text in 46 languages and 13 programming languages. It is the first multilingual LLM trained by more than 1,000 AI researchers, available for research and commercial purposes.                                                                                                                                                                                                                                                                                                                                                                                                    |
| [Fuyu](https://www.adept.ai/blog/fuyu-8b)                   | [ADEPT](https://www.adept.ai/)                                               | [Fuyu-8b](https://huggingface.co/adept/fuyu-8b)                                                                                                                                                                                                                                                                                        | Fuyu-8B, an ADEPT multimodal model, excels at text and image comprehension. It simplifies the traditional transformer architecture, making it easier to grasp, scale, and deploy. Fuyu-8B handles complex visual relationships, including charts and documents, and performs tasks like OCR and text localization in images.                                                                                                                                                                                                                                                                                                        |
| [PaLM 2](https://ai.google/discover/palm2/)                 | Google                                                                       | [PaLM 2](https://ai.google/discover/palm2/)                                                                                                                                                                                                                                                                                            | A superior language model by Google, excels in advanced reasoning, translation, and code generation. The next-gen PaLM is smaller yet more efficient, featuring enhanced performance with faster inference and reduced serving costs. Its diverse multilingual pre-training includes human and programming languages, equations, scientific papers, and web content. With improved architecture and varied task training, PaLM 2 caters to text generation, language translation, creative content creation, and informative question answering. [(blog)](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/) |
| [Qwen](https://github.com/QwenLM/Qwen)                      | Alibaba Cloud                                                                | [Qwen-1.8B](https://huggingface.co/Qwen/Qwen-1_8B), [Qwen-7B](https://huggingface.co/Qwen/Qwen-7B), [Qwen-14B](https://huggingface.co/Qwen/Qwen-14B), [Qwen-72B](https://huggingface.co/Qwen/Qwen-72B)                                                                                                                                 | Qwen, Alibaba Cloud's chat and pretrained large language model, showcases robust language models pre-trained on 3 trillion tokens of multilingual data, excelling in diverse tasks like chatting, content creation, information extraction, and more. Aligned with human preference through SFT and RLHF, Qwen delivers competitive performance with a focus on Chinese and English.                                                                                                                                                                                                                                                |
| [DeepSeek-LLM](https://github.com/deepseek-ai/DeepSeek-LLM) | [Deepseek](https://www.deepseek.com/)                                        | [DeepSeek-LLM 7B Base](https://huggingface.co/deepseek-ai/deepseek-llm-7b-base), [DeepSeek-LLM 7B Chat](https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat), [DeepSeek-LLM 67B Base](https://huggingface.co/deepseek-ai/deepseek-llm-67b-base), [DeepSeek-LLM 67B Chat](https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat) | DeepSeek LLM is an advanced language model with 67-B parameters. Trained from scratch on a massive 2 trillion-token dataset in English and Chinese, it outperforms Llama2 70B Base in reasoning, coding, math, and Chinese comprehension. DeepSeek LLM 7B/67B Base and DeepSeek LLM 7B/67B Chat are open source for research purposes.                                                                                                                                                                                                                                                                                              |
| [Yi](https://github.com/01-ai/Yi)                           | [01.AI](https://01.ai/)                                                      | [Yi-6B](https://huggingface.co/01-ai/Yi-6B), [Yi-34B](https://huggingface.co/01-ai/Yi-34B), [Yi-6B-Chat](https://huggingface.co/01-ai/Yi-6B-Chat), [Yi-34B-Chat](https://huggingface.co/01-ai/Yi-34B-Chat)                                                                                                                             | Yi, a series of large language models developed by 01.AI, includes bilingual base models (English/Chinese) with sizes Yi-6B and Yi-34B. Trained with 4K sequence length, they can extend to 32K during inference. These models excel in tasks like common-sense reasoning, reading comprehension, math, and code. Benchmarking shows their performance compared to other open-source models.                                                                                                                                                                                                                                        |
