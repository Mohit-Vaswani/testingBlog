# Auido & Music Generation

Audio generation is the production of sound or music using AI algorithms. These algorithms can compose music, mimic voices, or generate sound effects, contributing to industries like music composition, voice synthesis, and audio production.

<br/>

import Image from "next/image";

<div style={{ position: "relative" }}>
  <Image
    src="/static/img/memes/aiMusicMeme.png"
    alt="AI music meme"
    height="300"
    width="1000"
    priority
  />
</div>

### Courses

- [Hugging Face Audio course](https://huggingface.co/learn/audio-course/chapter0/introduction): This course equips learners with the ability to address diverse audio tasks like speech recognition, classification, and text-to-speech using transformers. It delves into audio data intricacies, explores different transformer architectures, and empowers participants to train their audio transformers by leveraging potent pre-trained models.

### Advancements

- [MuseNet](https://openai.com/research/musenet) (2019) is an AI system developed by OpenAI that can generate 4-minute musical compositions with 10 different instruments. It is a deep neural network that can combine styles from country to Mozart to the Beatles, and it can generate music in a wide range of genres.

- [Jukebox](https://openai.com/research/jukebox) (2020), an OpenAI generative music model, generates music based on genre, artist, and lyrics. It advances musical quality, but a gap with human-created music remains <a class="link" target="_blank" href="https://cdn.openai.com/papers/jukebox.pdf">(paper)</a>. You can listen to its music on <a class="link" target="_blank" href="https://jukebox.openai.com/">Jukebox Music</a>. They have also released the <a class="link" target="_blank" href="https://github.com/openai/jukebox/">source code</a>.

- [MusicLM: Generating Music From Text](https://google-research.github.io/seanet/musiclm/examples/) (2023) by Google Research generates high-fidelity music from text descriptions and melodies, transforming hummed melodies to match text captions. Google also released the <a class="link" target="_blank" href="https://huggingface.co/datasets/google/MusicCaps">MusicCaps dataset</a>. AudioLM, another framework, generates high-quality audio, maintaining speaker identity and prosody. Both systems are remarkable AI developments for music and audio generation. <a class="link" target="_blank" href="https://arxiv.org/abs/2301.11325">(paper)</a> 

- [AudioLM: a Language Modeling Approach to Audio Generation](https://google-research.github.io/seanet/audiolm/examples/) (2023): AudioLM, a Google language model, transcribes spoken language in real-time, accommodating multiple speakers and background noise. It supports various languages and accents, with applications in speech recognition, translation, voice assistants, and accessibility. Google has shared the code on GitHub for further development. It's a notable advancement in NLP <a class="link" target="_blank" href="https://arxiv.org/pdf/2209.03143v2.pdf">(paper)</a> <a class="link" target="_blank" href="https://blog.research.google/2022/10/audiolm-language-modeling-approach-to.html">(blog post)</a>.

- [AudioCraft](https://ai.meta.com/blog/audiocraft-musicgen-audiogen-encodec-generative-ai-audio/) (2023), a Meta AI tool, includes <a class="link" target="_blank" href="https://huggingface.co/spaces/facebook/MusicGen">MusicGen</a>, <a class="link" target="_blank" href="https://felixkreuk.github.io/audiogen/">AudioGen</a>, and <a class="link" target="_blank" href="https://ai.meta.com/blog/ai-powered-audio-compression-technique/">EnCodec</a> models. MusicGen generates music, AudioGen creates audio from text inputs, and EnCodec improves music quality. It simplifies generative audio models and offers open-source <a class="link" target="_blank" href="https://github.com/facebookresearch/audiocraft">code</a>, advancing AI audio and music generation for faster feedback in early prototyping.

- [Lyria](https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/) (2023) by DeepMind, in collaboration with YouTube, reveals its advanced music generation model, Lyria, and two innovative AI experiments. "Dream Track" strengthens connections between artists and fans on YouTube Shorts, while "Music AI tools" collaborates with creators to enrich artistic processes.

- [Stable Audio](https://stability.ai/research/stable-audio-efficient-timing-latent-diffusion), Stability AI's inaugural product, enables users to generate original music and sound effects by inputting a text prompt and duration. The high-quality audio output, in 44.1 kHz stereo, is produced using a latent diffusion for audio model trained on data from the <a class="link" target="_blank" href="https://www.audiosparx.com/">AudioSparx</a> music library. <a class="link" target="_blank" href=" https://www.stableaudio.com/">(website)</a> 

- [Qwen-Audio](https://github.com/QwenLM/Qwen-Audio) by Alibaba Cloud is a robust audio chatbot and pretrained large audio language model. It's crafted to comprehend and respond to natural language queries, offering readily available code and pre-trained weights for swift implementation. Qwen-Audio is integrated into Alibaba Cloud's suite of models, alongside <a class="link" target="_blank" href="https://github.com/QwenLM/Qwen-VL">Qwen-VL</a> (vision language) and <a class="link" target="_blank" href="https://github.com/QwenLM/Qwen">Qwen</a> (language).

### Reference

- [Chirp v1](https://suno-ai.notion.site/Chirp-v1-Examples-cc71e6c0c79f4e03acf39aa5d5a3dd09), developed by Suno, introduces notable improvements. It enhances audio quality, lets you select genres for your songs, supports over 50 languages, is 25% faster than <a class="link" target="_blank" href="https://suno-ai.notion.site/Bark-v0-Examples-e572bcfcdf65429c916d4c6dd8ae175b">Chirp v0</a>, and empowers you to customize song structure using metatags like <a class="link" target="_blank" href="https://en.wikipedia.org/wiki/Verse%E2%80%93chorus_form">Verseâ€“chorus</a>. These updates mark the next generation of text-to-music AI.

- [Bark](https://huggingface.co/docs/transformers/model_doc/bark), a transformative text-to-speech model by Suno, excels in multilingual speech synthesis and generates diverse audio elements, including music, ambient sounds, and nonverbal expressions like laughter, sighs, and tears. <a class="link" target="_blank" href="https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c">Bark Speaker Library (v2)</a>, sample prompts for generating voices in different languages and genders. <a class="link" target="_blank" href="https://suno-ai.notion.site/Bark-v0-Examples-e572bcfcdf65429c916d4c6dd8ae175b">(examples)</a> <a class="link" target="_blank" href="https://github.com/suno-ai/bark">(code)</a> <a class="link" target="_blank" href="https://huggingface.co/spaces/suno/bark">(live model)</a>

- [Camenduru's Audio ML Papers](https://github.com/camenduru#-audio-ml-papers):  The GitHub repositories encompass audio generation, music captioning, voice conversion, text-to-speech, and more. Additionally, this repository offers substantial insights into audio generation, fostering a rich learning experience.









