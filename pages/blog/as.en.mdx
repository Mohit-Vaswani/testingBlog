---
title: 'In-depth Fine Tuning: PEFT with LoRA & QLoRA'
date: JUN 1, 2023
description: So how does block() really work with React?
---

import Image from "next/image";

<div className="flex flex-col items-center gap-4">

# In-depth Fine Tuning: PEFT with LoRA & QLoRA

  <small>[Severus](https://aidenybai.com) JUN 1 2023</small>
</div>

---

## 

<div style={{ position: "relative" }}>
  <Image
    src="/static/img/blog-img/weights.png"
    alt="neural netowrk img"
    height="300"
    width="1000"
    priority
  />
</div>


In neural networks, the weight matrices consist of floating-point numbers, typically stored in the 32-bit floating-point data type. Computers internally represent these floating-point values using binary code, achieved by flipping bits for the sign, exponent, and significant. This binary representation is fundamental to accurately handle and process numerical data within the neural network architecture.

<br/>
<div style={{ position: "relative" }}>
  <Image
    src="/static/img/blog-img/floating_points.png"
    alt="neural netowrk img"
    height="300"
    width="1000"
    priority
  />
</div>


Consider this scenario with the number 7.567 and an extra digit in a 32-bit system. To reduce precision, a common approach is adjusting the data types. One option is shifting to half precision, utilizing only half the bits for number representation, cutting memory needs in half. However, there's a trade-off â€“ precision is compromised, as evident in the above case. 

Unlike the 32-bit setup, we can't express as many digits, leading to a faster accumulation of rounding errors and a subsequent decline in precision. It's crucial to recognize the compromise between conserving memory and maintaining accuracy when opting for different data types.